{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972ed71-bd8e-4de9-82ff-fd77d92e21d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Ultrasound Preprocessing & DataLoader Pipeline\n",
    "# ============================================\n",
    "\n",
    "# STEP 1: Install & Import Required Libraries\n",
    "!pip install opencv-python albumentations torch torchvision tqdm\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================\n",
    "# STEP 2: Configurations\n",
    "# ============================================\n",
    "\n",
    "DATA_PATH = \"/content/ultrasound_dataset\"   # <-- unzip Zenodo data here\n",
    "FRAME_SIZE = (224, 224)                    # resize resolution\n",
    "FRAMES_PER_VIDEO = 16                      # number of frames to sample\n",
    "BATCH_SIZE = 8\n",
    "VAL_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.1\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ============================================\n",
    "# STEP 3: Utility Functions\n",
    "# ============================================\n",
    "\n",
    "def sample_frames(video_path, num_frames=FRAMES_PER_VIDEO):\n",
    "    \"\"\"Load video and sample N frames uniformly.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    indices = np.linspace(0, total_frames-1, num_frames, dtype=np.int32)\n",
    "    \n",
    "    frames = []\n",
    "    for i in range(total_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if i in indices:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, FRAME_SIZE)\n",
    "            frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def sample_label_frames(label_video_path, num_frames=FRAMES_PER_VIDEO):\n",
    "    \"\"\"Same as sample_frames but for segmentation masks.\"\"\"\n",
    "    cap = cv2.VideoCapture(label_video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    indices = np.linspace(0, total_frames-1, num_frames, dtype=np.int32)\n",
    "    \n",
    "    frames = []\n",
    "    for i in range(total_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if i in indices:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            frame = cv2.resize(frame, FRAME_SIZE, interpolation=cv2.INTER_NEAREST)\n",
    "            frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "# ============================================\n",
    "# STEP 4: Dataset Class\n",
    "# ============================================\n",
    "\n",
    "class UltrasoundDataset(Dataset):\n",
    "    def __init__(self, video_dir, label_dir, transform=None, augment=False):\n",
    "        self.video_dir = video_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.video_files = sorted(os.listdir(video_dir))\n",
    "        self.label_files = sorted(os.listdir(label_dir))\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Define augmentations (only for training set)\n",
    "        self.aug = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = os.path.join(self.video_dir, self.video_files[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "        \n",
    "        frames = sample_frames(video_path)\n",
    "        labels = sample_label_frames(label_path)\n",
    "        \n",
    "        # Apply augmentation frame by frame\n",
    "        if self.augment:\n",
    "            aug_frames, aug_labels = [], []\n",
    "            for f, l in zip(frames, labels):\n",
    "                augmented = self.aug(image=f, mask=l)\n",
    "                aug_frames.append(augmented[\"image\"])\n",
    "                aug_labels.append(augmented[\"mask\"])\n",
    "            frames, labels = np.array(aug_frames), np.array(aug_labels)\n",
    "        \n",
    "        # Normalize\n",
    "        frames = frames.astype(np.float32) / 255.0\n",
    "        labels = labels.astype(np.int64)\n",
    "        \n",
    "        # To torch tensors\n",
    "        frames = torch.from_numpy(frames).permute(0, 3, 1, 2)  # (T, C, H, W)\n",
    "        labels = torch.from_numpy(labels)  # (T, H, W)\n",
    "        \n",
    "        return frames, labels\n",
    "\n",
    "# ============================================\n",
    "# STEP 5: Dataset Splitting\n",
    "# ============================================\n",
    "\n",
    "video_dir = os.path.join(DATA_PATH, \"videos\")\n",
    "label_dir = os.path.join(DATA_PATH, \"labels\")\n",
    "\n",
    "dataset = UltrasoundDataset(video_dir, label_dir, augment=False)\n",
    "total_len = len(dataset)\n",
    "val_len = int(total_len * VAL_SPLIT)\n",
    "test_len = int(total_len * TEST_SPLIT)\n",
    "train_len = total_len - val_len - test_len\n",
    "\n",
    "train_set, val_set, test_set = random_split(dataset, [train_len, val_len, test_len])\n",
    "\n",
    "# Enable augmentation only for training set\n",
    "train_set.dataset.augment = True\n",
    "\n",
    "# ============================================\n",
    "# STEP 6: DataLoaders\n",
    "# ============================================\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_loader)} batches, Val: {len(val_loader)} batches, Test: {len(test_loader)} batches\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 7: Quick Sanity Check\n",
    "# ============================================\n",
    "\n",
    "frames, labels = next(iter(train_loader))\n",
    "print(\"Frames shape:\", frames.shape)   # [B, T, C, H, W]\n",
    "print(\"Labels shape:\", labels.shape)   # [B, T, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2369f6e2-1994-4622-b4a8-f6ca5769996d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d875b532-124f-4f73-bc94-9995d40dc004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d010405-95d5-4213-836c-c340fa225f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
